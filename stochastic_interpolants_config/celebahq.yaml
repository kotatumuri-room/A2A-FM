defaults:
  - stochastic_interpolants
  - override datasets: celebahq
  - override val_datasets: celebahq
  - override model: unet_model
  - override optimizer: AdamW
  - override c_list: celebahq
  - _self_
timesteps: 100
modelname: celebahq_all_5
trainer:
  max_epochs: 100
dataloader:
  _target_: torch.utils.data.DataLoader
  batch_size: 32
val_dataloader:
  _target_: torch.utils.data.DataLoader
  batch_size: 32
alpha:
  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 3
# ckpt_versions_gt: [5,2,1,1,0]
K: 5
c_list: [0,1,2,3,4]
eval:
  device: cuda
  c_indecies: [0,1,2,3,4]
  ae_config: "${rootdir}/autoencoder_ckpts/first_stage_models/vq-f4/config.yaml"
  ae_ckpt: "${rootdir}/autoencoder_ckpts/first_stage_models/vq-f4/model.ckpt"
  timesteps: 10
  img_path: ${rootdir}/outputs/celeba_evaluation/${modelname}
  initial_path: "${rootdir}/datasets/celebADialogEmbbeded_eval"
  targc: [[0, 1, 0, 0, 5],
        [2, 0, 0, 2, 1],
        [0, 0, 0, 4, 2],
        [0, 0, 1, 0, 1],
        [4, 0, 0, 3, 3],
        [5, 0, 0, 2, 3],
        [5, 0, 3, 0, 2],
        [2, 0, 0, 3, 2],
        [0, 0, 0, 2, 4],
        [5, 0, 0, 1, 1],
        [0, 0, 3, 1, 3],
        [0, 0, 1, 2, 1],
        [0, 0, 0, 1, 2],
        [4, 0, 1, 1, 1],
        [5, 0, 1, 0, 1],
        [0, 0, 0, 5, 2],
        [0, 0, 1, 0, 3],
        [3, 0, 0, 3, 1],
        [0, 0, 0, 4, 4],
        [0, 0, 3, 3, 5],
        [4, 0, 0, 3, 5],
        [5, 0, 0, 4, 2],
        [0, 0, 4, 0, 2],
        [0, 0, 1, 5, 2],
        [0, 1, 1, 2, 5],
        [0, 4, 0, 0, 2],
        [3, 0, 0, 3, 3],
        [4, 0, 0, 1, 1],
        [0, 0, 2, 0, 2],
        [4, 0, 1, 0, 4]]